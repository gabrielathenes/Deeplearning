{"cells":[{"cell_type":"markdown","metadata":{"id":"wtb5ZO5djGKi"},"source":["# Small data and deep learning\n","This pratical session proposes to study several techniques to improve training performance in the challenging context where few data and resources are available."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7uHiVwA0rLzB"},"outputs":[],"source":["import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","import torch.utils.data as data\n","from torch.autograd import Variable\n","from copy import deepcopy\n","import torchvision\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","import torchvision.utils as vutils\n","import time\n","import sys\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import skorch\n","from skorch import NeuralNetClassifier\n","from skorch.callbacks import EarlyStopping, ProgressBar, LRScheduler, WarmRestartLR\n","\n","\n","# Sklearn\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import KFold\n"]},{"cell_type":"markdown","metadata":{"id":"jIk769ZfrLzC"},"source":["## Some helpers"]},{"cell_type":"markdown","metadata":{"id":"nkEk9hbxjGKk"},"source":["# Introduction\n","Assume we are in a context where few \"gold\" labeled data are available for training, say $\\mathcal{X}_{\\text{train}}\\triangleq\\{(x_n,y_n)\\}_{n\\leq N_{\\text{train}}}$, where $N_{\\text{train}}$ is small. A large test set $\\mathcal{X}_{\\text{test}}$ is available. A large amount of unlabeled data, $\\mathcal{X}_{\\text{nolabel}}$, is available. We also assume that we have a limited computational budget (e.g., no GPUs).\n","\n","For each question, write a commented *Code* or a complete answer as a *Markdown*. When the objective of a question is to report a CNN accuracy, please use the following format to report it, at the end of the question :\n","\n","\n","| Model | Number of  epochs  | Train accuracy | Test accuracy |\n","|------|------|------|------|\n","|   XXX  | XXX | XXX | XXX |\n","\n","\n","If applicable, please add the field corresponding to the  __Accuracy on Full Data__ as well as a link to the __Reference paper__ you used to report those numbers. (You do not need to train a CNN on the full CIFAR10 dataset.)\n","\n","In your final report, please keep the logs of each training procedure you used. We will only run this jupyter if we have some doubts on your implementation. \n","\n","__The total file sizes should be reasonable (feasible with 2MB only!). You will be asked to hand in the notebook, together with any necessary files required to run it if any.__\n","\n","\n","To run your experiments, you can use the same local installation as for previous TPs, or otherwise https://colab.research.google.com/."]},{"cell_type":"markdown","metadata":{"id":"yyz4hH-GjGKk"},"source":["## Training set creation\n","__Question 1 (2 points) :__ Propose a dataloader that will only use the first 100 samples of the CIFAR-10 training set. \n","\n","*Hint* : You can modify the file located at https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py or use the information from https://pytorch.org/vision/stable/datasets.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-5-cEvSQrLzE"},"outputs":[],"source":["class cifarloader():\n","    def __init__(self,train,download,transform, start, end):\n","        self.dataset = torchvision.datasets.CIFAR10(\"Data\", train = train, download = download, transform = transform)\n","        self.dataset.data = self.dataset.data[start:end]\n","        self.dataset.targets = self.dataset.targets[start:end]"]},{"cell_type":"markdown","metadata":{"id":"Sx8Y526AjGKl"},"source":["This is our dataset $\\mathcal{X}_{\\text{train}}$, it will be used until the end of this project. The remaining samples correspond to $\\mathcal{X}_{\\text{nolabel}}$. The testing set $\\mathcal{X}_{\\text{test}}$ corresponds to the whole testing set of CIFAR-10."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TuYW30YrrLzF"},"outputs":[],"source":["transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])"]},{"cell_type":"markdown","metadata":{"id":"kcFD3xMejGKm"},"source":["## Testing procedure\n","__Question 2 (1.5 points) :__ Explain why the evaluation of the training procedure is difficult. Propose several solutions."]},{"cell_type":"markdown","metadata":{"id":"jJhti4TUrLzG"},"source":[" The training procedure on small data is diffucult for different reasons. \n","- Problem 1 : Noise, missing values, wrong labels, outliers can lean to huge errors of our model's predictions on the test set. \n","> Solution 1 : Preprocess carefully the data so that it is as clean as possible. Since the data is small, and assuming that we are in the case where we are able to recognize the label of our data (possible with CIFAR images), wrong labels, missing values and outliers can be preprocessed manually. If it's not the case, we could for example do unsupervised clustering on our data to visualize outliers and wrong labels. In our case the data is already clean.\n","\n","- Problem 2 : Our trainset might not be representative of the whole data wheteher be it due to a sampling bias or just chance. For example, let's assume that we have a small dataset of $m$ points, and that the large dadaset has $n$ classes of equal proportions. Then the probability that a class is absent from the small dataset is $1$ if $m<n$ and $\\leq n(\\frac{n-1}{n})^{m}$ otherwise. In our case, with $n=10$ and $m=100$, this probability is less than $0.03\\%$. \n","> Solution 2.1 : We could first create some metrics in order to assess the representativity of our model. This would give an idea of how much we can trust our model to make predictions on a bigger dataset. A metric would be for example the distance between the sorted labels of our small dataset $L_1 \\dots L_m$ and the sorted labels of a dataset with equal proportions of each label $L_1^{*} \\dots L_m^{*}$. \n","> Solution 2.2 : We can use data augmentation techniques. \n","\n","- Problem 3 : Difficult not to overfit. \n","> Solution 3 :Be careful of the parameters such as the number of epochs. Do cross validation so that every part of the set is used to train and test.\n","\n","- Problem 4 : Cross validation can lead to aberrant values for the hyper-parameters if the validation sets are taken too small. The same happens if the training sets are taken too small too. \n","> Solution 4 : In our case, we do cross validation with only 3 splits.\n"]},{"cell_type":"markdown","metadata":{"id":"b8yeRD7CjGKm"},"source":["# Raw approach: the baseline"]},{"cell_type":"markdown","metadata":{"id":"4dgKvuS0jGKm"},"source":["In this section, the goal is to train a CNN on $\\mathcal{X}_{\\text{train}}$ and compare its performance with reported numbers from the litterature. You will have to re-use and/or design a standard classification pipeline. You should optimize your pipeline to obtain the best performances (image size, data augmentation by flip, ...).\n","\n","The key ingredients for training a CNN are the batch size, as well as the learning rate schedule, i.e. how to decrease the learning rate as a function of the number of epochs. A possible schedule is to start the learning rate at 0.1 and decreasing it every 30 epochs by 10. In case of divergence, reduce the learning rate. A potential batch size could be 10, yet this can be cross-validated.\n","\n","You can get some baselines accuracies in this paper : http://openaccess.thecvf.com/content_cvpr_2018/papers/Keshari_Learning_Structure_and_CVPR_2018_paper.pdf. Obviously, it is a different context for those researchers who had access to GPUs."]},{"cell_type":"markdown","metadata":{"id":"pJpkBpyWjGKn"},"source":["## ResNet architectures"]},{"cell_type":"markdown","metadata":{"id":"z22FjUNYjGKn"},"source":["__Question 3 (4 points) :__ Write a classification pipeline for $\\mathcal{X}_{\\text{train}}$, train from scratch and evaluate a *ResNet-18* architecture specific to CIFAR10 (details about the ResNet-18 model, originally designed for the ImageNet dataset, can be found here: https://arxiv.org/abs/1512.03385). Please report the accuracy obtained on the whole dataset as well as the reference paper/GitHub link.\n","\n","*Hint:* You can re-use the following code: https://github.com/kuangliu/pytorch-cifar. During a training of 10 epochs, a batch size of 10 and a learning rate of 0.01, one obtains 40% accuracy on $\\mathcal{X}_{\\text{train}}$ (\\~2 minutes) and 20% accuracy on $\\mathcal{X}_{\\text{test}}$ (\\~5 minutes)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h9wmxgUSjGKn"},"outputs":[],"source":["# code copied from  https://github.com/kuangliu/pytorch-cifar.\n","\n","'''ResNet in PyTorch.\n","For Pre-activation ResNet, see 'preact_resnet.py'.\n","Reference:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(\n","            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion *\n","                               planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(BasicBlock, [2, 2, 2, 2])\n","\n","def test():\n","    net = ResNet18()\n","    y = net(torch.randn(1, 3, 32, 32))\n","    print(y.size())\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qHyQtbeZrLzI"},"outputs":[],"source":["class Classification_pipeline():\n","    def __init__(self, net, transform_train, transform_test, params):\n","        \n","        self.data_train = cifarloader(train = True, download = True, transform = transform_train, start = 0, end = 100)\n","        self.data_test = cifarloader(train = False, download = True, transform = transform_test, start = None, end = None)\n","        \n","        self.train_loader = torch.utils.data.DataLoader(self.data_train.dataset, batch_size=params['batchsize'], shuffle=True, num_workers=2)\n","        self.test_loader = torch.utils.data.DataLoader(self.data_test.dataset, batch_size=params['batchsize'], shuffle=True, num_workers=2)\n","        \n","        self.params=params\n","        \n","        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","        \n","        self.net = net.to(self.device)\n","        \n","        if self.device == 'cuda':\n","            self.net = torch.nn.DataParallel(self.net)\n","            cudnn.benchmark = True\n","\n","        self.criterion = nn.CrossEntropyLoss()\n","        self.optimizer = optim.SGD(net.parameters(), \n","                                   lr=params['lr'],\n","                                   momentum=params['momentum'],\n","                                   weight_decay = params['weight_decay'])\n","        \n","        self.best_acc = 0\n","        \n","    def update_params(self,params):\n","        self.params ['lr'] = params['lr']\n","        self.params['batch_size'] = params ['batch_size']\n","        self.params['max_epochs'] = params['max_epochs']\n","        \n","    def update_optimizer(self):\n","        self.optimizer = optim.SGD(self.net.parameters(), \n","                                   lr=self.params['lr'],\n","                                   momentum=self.params['momentum'],\n","                                   weight_decay = self.params['weight_decay'])\n","        \n","    def gridsearch(self,grid_parameters):\n","        X_train, y_train= self.data_train.dataset.data, self.data_train.dataset.targets\n","        GridNet = NeuralNetClassifier(\n","            self.net.double(),\n","            iterator_train__shuffle=True,\n","            criterion=nn.CrossEntropyLoss,\n","            optimizer=torch.optim.SGD,\n","            device=self.device)\n","\n","        grid = GridSearchCV(GridNet, grid_parameters, refit=True, cv=KFold(n_splits=3), scoring='accuracy')\n","        start_time = time.time()\n","        grid.fit(np.array([X.reshape(3, 32, 32).astype('double') for X in X_train]), np.array(y_train).astype('int64'))\n","        \n","        print('Cross-validation finished in ' + str(time.time()-start_time) + ' seconds.')\n","        print('Best accuracy is '+ str(grid.best_score_))\n","        print('Best parameters are '+ str(grid.best_params_))\n","        self.update_params(grid.best_params)\n","        self.update_optimizer()\n","        \n","        self.net.float()\n","        \n","        \n","        return grid.best_params\n","        \n","    # Training\n","    def train_epoch(self,epoch):\n","        net.train()\n","        train_loss = 0\n","        correct = 0\n","        total = 0\n","        for batch_idx, (inputs, targets) in enumerate(self.train_loader):\n","            inputs, targets = inputs.to(self.device), targets.to(self.device)\n","            self.optimizer.zero_grad()\n","            outputs = self.net(inputs)\n","            loss = self.criterion(outputs, targets)\n","            loss.backward()\n","            self.optimizer.step()\n","\n","            train_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","        loss, acc =  train_loss / (batch_idx+1), 100 * correct / total\n","        print('\\nEpoch %d\\nTrain_loss: %.3f | Train_acc: %.3f%% | ' % (epoch+1, loss, acc)) \n","\n","\n","    def test(self):\n","        \n","        net.eval()\n","        test_loss = 0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for batch_idx, (inputs, targets) in enumerate(self.test_loader):\n","                inputs, targets = inputs.to(self.device), targets.to(self.device)\n","                outputs = self.net(inputs)\n","                loss = self.criterion(outputs, targets)\n","\n","                test_loss += loss.item()\n","                _, predicted = outputs.max(1)\n","                total += targets.size(0)\n","                correct += predicted.eq(targets).sum().item()\n","            loss, acc =  test_loss / (batch_idx+1), 100 * correct / total\n","            print('\\nTest_loss: %.3f | Test_acc: %.3f%% | ' % (loss, acc)) \n","\n","        # Save checkpoint.\n","        \"\"\"acc = 100.*correct/total\n","        if acc > self.best_acc:\n","            print('Saving..')\n","            state = {\n","                'net': net.state_dict(),\n","                'acc': acc,\n","                'epoch': epoch,\n","            }\n","            if not os.path.isdir('checkpoint'):\n","                os.mkdir('checkpoint')\n","            torch.save(state, './checkpoint/ckpt.pth')\n","            self.best_acc = acc\"\"\"\n","    \n","    def train(self):\n","        for epoch in range(self.params['max_epochs']):\n","            if epoch+1 % 10 == 0 :\n","                self.optimizer.lr /=10\n","            self.train_epoch(epoch)\n","        self.test()\n","\n","            "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-tLgN68mrLzJ"},"outputs":[],"source":["net18 = ResNet18()\n","params_Resnet18 = {'lr' : 0.05, 'momentum' : 0.9,  'weight_decay' : 5e-4, 'batchsize' : 10, 'max_epochs' : 10 }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9viVuEYLrLzJ","outputId":"c642be50-605e-43e7-d3d0-a4ccf9e47355"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}],"source":["classifier_ResNet18 = Classification_pipeline(net18, transform_train, transform_test, params_Resnet18)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7x8ak-IorLzK"},"outputs":[],"source":["grid_parameters = {'lr': [0.005, 0.01, 0.05],\n","              'batch_size': [5, 10, 15],\n","              'max_epochs': [10, 20, 30, 40]}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LETvhtJJrLzK","outputId":"ab23a7ac-8359-4e39-e24f-e592bb0ef4e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.4033\u001b[0m       \u001b[32m0.0714\u001b[0m        \u001b[35m2.3087\u001b[0m  7.4602\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11712/3279023876.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclassifier_ResNet18\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgridsearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_parameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mclassifier_ResNet18\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11712/2796338056.py\u001b[0m in \u001b[0;36mgridsearch\u001b[1;34m(self, grid_parameters)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGridNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'double'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'int64'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cross-validation finished in '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' seconds.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    889\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1390\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1392\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    836\u001b[0m                     )\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m                 out = parallel(\n\u001b[0m\u001b[0;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[0;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    862\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[0mtest_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_score\u001b[1;34m(estimator, X_test, y_test, scorer, error_score)\u001b[0m\n\u001b[0;32m    759\u001b[0m             \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m             \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    762\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merror_score\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raise\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[0mScore\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mapplied\u001b[0m \u001b[0mto\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0mof\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0mon\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \"\"\"\n\u001b[1;32m--> 216\u001b[1;33m         return self._score(\n\u001b[0m\u001b[0;32m    217\u001b[0m             \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_cached_call\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m_score\u001b[1;34m(self, method_caller, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[0;32m    256\u001b[0m         \"\"\"\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod_caller\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"predict\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             return self._sign * self._score_func(\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m_cached_call\u001b[1;34m(cache, estimator, method, *args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;34m\"\"\"Call estimator with method and args and kwargs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\skorch\\classifier.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \"\"\"\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\skorch\\classifier.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;31m# Only the docstring changed from parent.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;31m# pylint: disable=useless-super-delegation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\skorch\\net.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1431\u001b[0m         \u001b[0mnonlin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_predict_nonlinearity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m         \u001b[0my_probas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1433\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0myp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1434\u001b[0m             \u001b[0myp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0myp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m             \u001b[0myp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnonlin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\skorch\\net.py\u001b[0m in \u001b[0;36mforward_iter\u001b[1;34m(self, X, training, device)\u001b[0m\n\u001b[0;32m   1277\u001b[0m         \u001b[0miterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1278\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1279\u001b[1;33m             \u001b[0myp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1280\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mto_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\skorch\\net.py\u001b[0m in \u001b[0;36mevaluation_step\u001b[1;34m(self, batch, training)\u001b[0m\n\u001b[0;32m   1034\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\skorch\\net.py\u001b[0m in \u001b[0;36minfer\u001b[1;34m(self, x, **fit_params)\u001b[0m\n\u001b[0;32m   1357\u001b[0m             \u001b[0mx_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_merge_x_and_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mx_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1359\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_predict_nonlinearity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11712/1007743219.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11712/1007743219.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ENV38\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 442\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["classifier_ResNet18.gridsearch(grid_parameters)\n","classifier_ResNet18.net.float()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PIl8rYZSrLzL"},"outputs":[],"source":["classifier_ResNet18.train()"]},{"cell_type":"markdown","metadata":{"id":"JLv78we5jGKo"},"source":["# Transfer learning"]},{"cell_type":"markdown","metadata":{"id":"et0xds0xjGKo"},"source":["We propose to use pre-trained models on a classification task, in order to improve the results of our setting."]},{"cell_type":"markdown","metadata":{"id":"ErqP4xNMjGKo"},"source":["## ImageNet features"]},{"cell_type":"markdown","metadata":{"id":"CyHaYiMCjGKo"},"source":["Now, we will use a model pre-trained on ImageNet and see how well it performs on CIFAR. A list of ImageNet pre-trained models is available on : https://pytorch.org/vision/stable/models.html\n","\n","__Question 4 (3 points) :__ Pick a model from the list above, adapt it for CIFAR and retrain its final layer (or a block of layers, depending on the resources to which you have access to). Report its accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DQu_qudYjGKo"},"outputs":[],"source":["EfficientNetB0_pretrained = models.efficientnet_b0(pretrained=True)\n","\n","for param in EfficientNetB0_pretrained.parameters():\n","  param.requires_grad = False\n","\n","#visualize model\n","#print(EfficientNetB0_pretrained)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hp97ywXfrLzM"},"outputs":[],"source":["EfficientNetB0_pretrained.classifier = nn.Sequential(\n","                        nn.Dropout(0.2),\n","                        nn.Linear(1280, 10))                        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tNbDeDTgrLzM"},"outputs":[],"source":["params_EfficientNet = {'lr' : 0.05, 'momentum' : 0.9,  'weight_decay' : 5e-4, 'batchsize' : 10, 'max_epochs' : 10}\n","\n","classifier_EfficientNet = Classification_pipeline(EfficientNetB0_pretrained, transform_train, transform_test, params_EfficientNet)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LB3vLVdCrLzM"},"outputs":[],"source":["classifier_EfficientNet.gridsearch(grid_parameters)\n","classifier_EfficientNet.net.float()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AEEIVsELrLzM"},"outputs":[],"source":["classifier_EfficientNet.train()"]},{"cell_type":"markdown","metadata":{"id":"-svmb56SjGKp"},"source":["# Incorporating priors\n","Geometrical priors are appealing for image classification tasks. \n","A color image $x$ can be seen as a function: $\\mathbb{S}\\rightarrow\\mathbb{R}^3$, where $\\mathbb{S} \\subset \\mathbb{R}^2$ is the image support.\n","Let us consider transformations $\\mathcal{T}$ of possible inputs $x$. For instance, if an image had infinite support, a translation $\\mathcal{T}_a$ of an image $x$ by a shift $a$ would lead to a new infinite-support image $\\mathcal{T}_a(x)$, described at each pixel $u$ by :\n","\n","$$\\forall u, \\mathcal{T}_a(x)(u)=x(u-a)\\,.$$\n","\n","__Question 5 (1.5 points) :__ Explain the issues when dealing with translations, rotations, scaling effects, color changes on $32\\times32$ images. Propose several ideas to tackle them."]},{"cell_type":"markdown","metadata":{"id":"9YJCU9PgjGKp"},"source":["\n","\n","- Problem with translating & scaling : We can miss important objects of the picture that would be used for classification. If we translate and pad, then the borders between the black padding zone and the image could help the model to classify in a wrong way \n","> Solution for translation : Limit the translation vector to a few pixels.\n","> Solution for scaling : Bound the scaling factor\n","> Solution for padding borders : Zoom and translate. (advised in pytorch documentation)\n","             \n","- Problem with rotating : The square is invariant to only 4 rotations. Any rotation that is not a multiple of $\\pi/2$ leads to black padding zones on the image.\n","> Solution : Limit rotations to multiple of $\\pi/2$.\n","> Solution : Zoom and rotate. The zooming factor needs to be larger than $\\sqrt{2}$, meaning that we lose a lot of information on the image. \n","\n","- Problems with color changes : For some images, the colors of the background can provide the model crucial information on the label. Let's take an irrealistic example. Let's assume our model tries to differentiate asian and african elefants. What might differ between these two images is the landscape in the background. Adding pictures with color changes can confuse our model and prevent him from using information on these background colors.\n","\n","> Solution: use very light color changes. We could on the other hand to local histogram equalization."]},{"cell_type":"markdown","metadata":{"id":"CxMnR6QNjGKp"},"source":["## Data augmentations"]},{"cell_type":"markdown","metadata":{"id":"vvohBWdmjGKp"},"source":["__Question 6 (3 points) :__ Propose a set of geometric transformations beyond translation, and incorporate them in your training pipeline. Train the model of the __Question 3__ with them and report the accuracies.\n","You can use tools from https://pytorch.org/vision/stable/transforms.html "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PXA1-KP2jGKp"},"outputs":[],"source":["transform_train_aug = transforms.Compose([\n","        \n","        #scale & translation \n","        transforms.Resize((38,38)),\n","        transforms.RandomResizedCrop(32, scale=(0.8, 1.0)),\n","        transforms.RandomAffine(10),\n","        transforms.RandomPerspective(),\n","\n","        #rotation\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomVerticalFlip(),\n","    \n","        #color changes\n","        transforms.RandomAutocontrast(),\n","        transforms.RandomEqualize(),\n","        \n","        transforms.ToTensor(),        \n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","    ])\n","\n","transform_test_aug = transforms.Compose([\n","    transforms.Resize((32,32)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z3s-90UErLzN","outputId":"55132b3b-0ef9-472b-855f-3a2a37d865e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}],"source":["classifier_ResNet18_aug = Classification_pipeline(net, transform_train_aug, transform_test_aug, params_Resnet18)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tJ8NAqgDrLzN"},"outputs":[],"source":["classifier_ResNet18_aug.gridsearch(grid_parameters)\n","classifier_ResNet18_aug.net.float() #only necessary if you stop the grid search manually"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4U5qFqPDrLzN"},"outputs":[],"source":["classifier_ResNet18_aug.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"orRCz_tCrLzN"},"outputs":[],"source":["classifier_EfficientNet_aug = Classification_pipeline(EfficientNetB0_pretrained, transform_train_aug, transform_test_aug, params_EfficientNet)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXudAsZ8rLzO"},"outputs":[],"source":["classifier_EfficientNet_aug.gridsearch(grid_parameters)\n","classifier_EfficientNet_aug.net.float()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vkVSQDFSrLzO"},"outputs":[],"source":["classifier_EfficientNet_aug.train()"]},{"cell_type":"markdown","metadata":{"id":"1I81WSECjGKq"},"source":["# Conclusions"]},{"cell_type":"markdown","metadata":{"id":"xjQRkH-kjGKq"},"source":["__Question 7 (5 points) :__ Write a short report explaining the pros and the cons of each method you implemented. 25% of the grade of this project will correspond to this question, thus, it should be done carefully. In particular, please add a plot that will summarize all your numerical results."]},{"cell_type":"markdown","metadata":{"id":"0j5T6KnjrLzO"},"source":["# Short report\n","\n","**1. Introduction** \n","\n","> The goal of this practical session was to train a supervised model on very small data. We therefore use as a training set the first 100 samples of the CIFAR10 training dataset. We will test our models on the CIFAR10 tesing set which consists of 10 000 samples. The methodology of this TP is the following : We first train a model from scratch (ResNet18), incorporating some solutions to the problems of small data training that we discussed in the first question. We then train the last layer only of a pretrained model, EfficientNetb0. We then train these same models after having performed data augmentation on the small data to assess it's. We summarize the performances of each model.\n","\n","\n","\n","**2. TrainingResNet18 from scratch on a small dataset**\n","\n","\n","ResNet-18 is a residual network with 18 layers.\n","\n","\n","I optimized the weights using Stochastic Gradient Descent and the hyper parameters of a Grid Search. These parameters turned out to be a learning rate of 0.005, a momentum of 0.9, a batch size of 5, number of epochs to 10. I devided the learning rate by 10 every  epochs to avoid over-fitting. The accuracy is $21.91\\%$ on the test set.\n","\n",">Pros : With small data,  Grid Search is feasible with reasonable training time using any of Google Colab's GPUs. Training is also very fast with GPU. \n","\n",">Cons : There is a lot of overfitting. There are too many parameters to learn the biggest features of our data. On larger datasets, this model becomes more accurate but takes more time to train.  \n","\n","**3. Training the last layer of EfficientNet on a small dataset**\n","\n","EfficientNet is a lightweight convolutional neural network architecture achieving the state-of-the-art accuracy that rely on AutoML and compound scaling. As it has an order of magnitude less parameters, I thought it was perfectly adapted to the small dataset problem as it should help reduce overfitting. I optimized the weights using Stochastic Gradient Descent and the hyper parameters of a Grid Search. These parameters turned out to be a learning rate of 0.05, a momentum of 0.9, a batch size of 15, number of epochs to 10. I devided the learning rate by 10 every 20 epochs to avoid over-fitting. The accuracy is $16.41\\%$ on the test set.\n","\n",">Pros : Less parameters helps reduce overfitting.  Less training time as only a single layer is trained. As the model is pretrained and has already learned the bigger features of the images, parameter tuning can be performed only on the last layer, meaning grid search can be performed in correct execution time.\n","\n",">Cons : We need the pretrained model for the architecture, so we cannot use this method for any architecture we want.\n","\n","**4. Augmenting the dataset with transform techniques**\n","\n","\n","As we have a small number of samples, we used data augmentation.\n","\n","I tried translations and scalings, rotations with multiples of $\\pi/2$, color changes, normalization. \n","\n",">Pros : This helps our model to learn new types of inputs that might be present in the test set. \n","\n",">Cons : Looking for the optimal transformations takes time, as the optimal transformations are often counter-intuitive. This requires a good understanding of our data. We also cannot perform gridsearch as we are directly changing the data and not parameters of our model. I did not manage to find transformations that increased the accuracy of our model. This is probably because the most important transformations are the ones I had already used in the first questions.\n","\n","**4. Summary**\n","\n","| Model | Number of  epochs  | Train accuracy | Test accuracy |\n","|------|------|------|------|\n","|   ResNet 18  |10| $36\\%$ | $21.91\\%$ |\n","|   ResNet 18 augmented  |10| $49\\%$ | $21.68\\%$ |\n","|   EfficientNet_pretrained  |40| $48\\%$ | $16.41\\%$ |\n","|   EfficientNet_pretrained_augmented  |40| $44\\%$ | $16.40\\%$ |\n","\n","**4. Conclusion**\n","\n","I am aware that my experiments are not very conclusive as I failed to show that some used methods such as Data augmentation or transfer learning can improve test accuracy. On the other hand even though I lacked the time to use these tools efficiently in this TP, I learned how to implement them and will not hesitate to use them when I am confronted to small data problems and have time to improve my models. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t8K821e3rLzO"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"8MIfKUUojGKq"},"source":["# Weak supervision"]},{"cell_type":"markdown","metadata":{"id":"RldQ1hdPjGKq"},"source":["__Bonus \\[open\\] question (up to 4 points) :__ Pick a weakly supervised method that will potentially use $\\mathcal{X}_{\\text{nolabel}}\\cup\\mathcal{X}_{\\text{train}}$ to train a model (a subset of $\\mathcal{X}_{\\text{nolabel}}$ is also fine). Evaluate it and report the accuracies. You should be careful in the choice of your method, in order to avoid heavy computational effort."]},{"cell_type":"markdown","metadata":{"id":"5dZUwWXKrLzP"},"source":["In this questio I will use a semi-supervised model that we designed in another course, GraphsML. This question enables me to test this model in the case where the dataset is not the same one proposed by the professor. The next cell is a paragraph explaining this model and __entirely__ written by our professor. "]},{"cell_type":"markdown","metadata":{"id":"j1TiXqCerLzP"},"source":["## 1. Harmonic Function Solution\n","\n","Let $G = (V, E)$ be a weighted undirected graph where $V = \\{x_1, \\ldots, x_n \\}$ is the vertex set and $E$ is the edge set. Each edge $e_{ij} \\in E$ has a weight $w_{ij}$ and, if there is no edge between $x_i$ and $x_j$, then $w_{ij}=0$.\n","\n","Let $|V| = n$ be the total number of nodes. Only a subset of the nodes $S \\subset V$ with cardinality $|S| = l$ is labeled, and the remaining $u = n - l$ nodes are placed in the subset $T = V \\setminus S$. \n","\n","Our goal is to predict the labels of the vertices in $T$ using the structure of the graph. Since we believe that nodes close in the graph should have similar labels, we would like to have each node surrounded by a majority of nodes with the same label. In order to do so, we impose that the labeling vector $f \\in \\mathbb{R}^n$ must be an **harmonic function** on the graph, that is:\n","\n","$$\n","f_i = \\frac{\\sum_{j} w_{ij} f_j}{\\sum_{j} w_{ij}},  \\forall i \\in T\n","$$\n","\n","One interpretation for this constraint is that $w_{ij}$ represents the tendency of moving from node $x_i$ to node $x_j$, the stationary distribution of the transition matrix $P(j|i) = \\tfrac{w_{ij}}{\\sum_{k} w_{ik}}$  is a valid solution to our problem. \n","\n","### Hard HFS\n","\n","It can be shown that $f$ is harmonic if and only if $(Lf)_T = 0$, where $(Lf)_T$ is the vector containing the values of $Lf$ for the nodes in the set $T$, and $L$ is the graph Laplacian. \n","\n","Hence, the harmonic function solution to the SSL problem is the solution to the following optimization problem:\n","\n","$$\n","\\min_{f \\in \\mathbb{R}^n}  f^T L f  \n","\\quad \\text{s.t} \\quad\n","y_i = f(x_i) \\quad \\forall x_i \\in S\n","$$\n","where $y_i$ are the labels available for the vertices $x_i \\in S$. This gives us:\n","\n","$$\n","f_T = L_{TT}^{-1}(W_{TS}f_S) = - L_{TT}^{-1}(L_{TS}f_S) \n","$$\n","\n","### Soft HFS\n","\n","If the labels are noisy, we might need to replace the \"hard\" constraint of the optimization problem above by a \"soft\" constraint. Let $C$ be a diagonal matrix such that $C_{ii} = c_l$ for labeled examples and $C_{ii} = c_u$ otherwise. Also, define $y_i = 0$ for unlabeled examples, that is, for $x_i \\in T$. \n","\n","The soft HFS objective function is\n","\n","$$\n","\\min_{f\\in\\mathbb{R}^n} (f-y)^T C (f-y) + f^T L f\n","$$\n","whose solution is \n","\n","$$\n","f^* = (C^{-1}L+I)^{-1}y\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Opz1wsAHrLzP"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.spatial.distance as sd\n","from scipy.io import loadmat\n","import os\n","from helper import build_similarity_graph, label_noise\n","from helper import build_laplacian, build_laplacian_regularized\n","from helper import plot_classification\n","from helper import mask_labels\n","from sklearn.utils import shuffle\n","import time\n","from sklearn.neural_network import MLPClassifier\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x47kK4AcrLzP"},"outputs":[],"source":["\"\"\"\n","Define parameters for face recognition with HFS\n","\"\"\"\n","params_ssl = {}\n","params_ssl['laplacian_regularization'] = 0.75\n","params_ssl['var'] = 10.0\n","params_ssl['eps'] = 1.\n","params_ssl['k'] = None\n","params_ssl['laplacian_normalization'] = 'unn'\n","params_ssl['c_l'] = 0.9\n","params_ssl['c_u'] = 0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WKWYABiUrLzP"},"outputs":[],"source":["def compute_hfs(L, Y, soft=False, **params):\n","    \"\"\"\n","    Parameters\n","    ----------\n","    L : array\n","        Graph Laplacian, (n x n) matrix (regularized or not)\n","    Y : array\n","        (n, ) array with nodes labels [0, 1, ... , num_classes] (0 is unlabeled)\n","    soft : bool\n","        If True, compute soft HFS. Otherwise, compute hard HFS.\n","\n","    Returns\n","    --------\n","        Labels, class assignments for each of the n nodes\n","    \"\"\"\n","\n","    num_samples = L.shape[0]\n","    Cl = np.unique(Y)\n","    num_classes = len(Cl)-1\n","\n","    \"\"\"\n","    Build the vectors:\n","    y = (n x num_classes) target vector \n","    l_idx = shape (l,) vector with indices of labeled nodes\n","    u_idx = shape (u,) vector with indices of unlabeled nodes\n","    \"\"\"\n","    # ...\n","    l_idx = Y != 0\n","    u_idx = Y ==0\n","    \n","    # we encode using the solution provided here https://stackoverflow.com/questions/38592324/one-hot-encoding-using-numpy \n","    target_indices = (Y[l_idx]-1).astype(int)    \n","    y = np.zeros((num_samples,num_classes))\n","    M = 2*np.eye(num_classes)-np.ones((num_classes,num_classes))\n","    \n","    y[l_idx]= M[target_indices]\n","    \n","    if not soft:    \n","        \"\"\"\n","        Compute hard HFS.  \n","\n","        f_l = solution for labeled data. \n","        f_u = solution for unlabeled data\n","        f   = solution for all data\n","        \"\"\"\n","        \n","        f_l = M[target_indices]\n","        f_u = - np.linalg.inv(L[:,u_idx][u_idx])@(L[:,l_idx][u_idx]@f_l)\n","        f = np.zeros((num_samples,num_classes))\n","        f[l_idx]=f_l\n","        f[u_idx]=f_u\n","        \n","    else:\n","        \"\"\"\n","        Compute soft HFS.\n","        f = harmonic function solution \n","        C = (n x n) diagonal matrix with c_l for labeled samples and c_u otherwise    \n","        \"\"\"\n","        u = np.zeros(L.shape[0]) \n","        u[l_idx] = params['c_l']\n","        u[u_idx] = params['c_u']\n","        C = np.diag(u)         \n","        f = np.linalg.inv(np.linalg.inv(C)@L + np.eye(num_samples))@y \n","\n","    \"\"\"\n","    return the labels assignment from the hfs solution, and the solution f\n","    labels: (n x 1) class assignments [1,2,...,num_classes]    \n","    f : harmonic function solution\n","    \"\"\"\n","    labels = np.argmax(f,axis=1)+1\n","    return labels, f\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5SEFNHghrLzP","outputId":"5bffb777-53ba-4a67-ad1a-3f75770f7f53"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n"]}],"source":["transform_ssl = transforms.Compose([\n","    transforms.RandomAutocontrast(),\n","    transforms.RandomEqualize(),\n","    transforms.Grayscale(),\n","    transforms.GaussianBlur(kernel_size=(9,9)),\n","    transforms.ToTensor(),\n","])\n","\n","size = 2000\n","data_train_full = cifarloader(train = True, download = True, transform = transform_ssl, start = None, end = size )\n","\n","\n","X_train, y_train= data_train_full.dataset.data, data_train_full.dataset.targets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bz-kF--grLzQ"},"outputs":[],"source":["np.random.seed(15)\n","X_train_shuffled, y_train_shuffled = shuffle(X_train,y_train)\n","X_train_shuffled = X_train_shuffled.reshape(size,-1)/255\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ntQ26UAYrLzQ"},"outputs":[],"source":["time_start = time.time()\n","L = build_laplacian_regularized(X_train_shuffled, \n","                                params_ssl['laplacian_regularization'], \n","                                params_ssl['var'], \n","                                params_ssl['eps'], \n","                                params_ssl['k'], \n","                                params_ssl['laplacian_normalization'])"]},{"cell_type":"markdown","metadata":{"id":"d-XnzmpqrLzQ"},"source":["__Now let's plot the performance of the model with respect to the size of the training set.__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GJjZz7HHrLzQ","outputId":"7e2e44ea-950b-455b-e507-ace8ce9808ae"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:02<00:00,  7.80it/s]\n"]},{"name":"stdout","output_type":"stream","text":["execution time =  249.538245677948\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 720x576 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# Run HFS\n","\n","test_labels = np.zeros(size)\n","ones=np.ones(size)\n","accuracy_semi_supervised = []\n","for i in tqdm(np.linspace(0,1,21)[1:-1]) :\n","    ind = int(i*size)\n","    test_labels[:ind]=y_train_shuffled[:ind]+ones[:ind] \n","    predicted_labels_unsupervised, f_a = compute_hfs(L, test_labels, soft=False, **params_ssl)\n","    accuracy_augmented = np.equal(predicted_labels_unsupervised[ind:], y_train_shuffled[ind:]+ones[ind:]).mean()\n","    accuracy_semi_supervised.append(accuracy_augmented)\n","\n","print('execution time = ', time.time()-time_start)\n","\n","x = 100*np.linspace(0,1,21)[1:-1]\n","fig, ax = plt.subplots(figsize=(10,8))\n","ax.plot(x,accuracy_semi_supervised,label='Semi-supervised')\n","ax.set(xlabel='Percent of whole set used for train set', ylabel='accuracy')\n","ax.plot()\n","ax.grid()\n","ax.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"x9O-WWvsrLzQ"},"source":["Semi supervised learning seems to perform correctly on very small training sets, with accuracies above $20\\%$ when $5\\%$ of the dataset is used, but fails to give good accuracies for larger training sets given the amount of data we have. (less than $30 \\%$ accuracy when the training set is $80 \\%$ of the whole set.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hqVbmB2brLzQ"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"name":"TP4_transfer.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":0}
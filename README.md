# Deeplearning

Despite impressive mediatized results, deep learning methods are still poorly understood, neural networks are often difficult to train, and the results are black-boxes missing explanations, which is problematic given the societal impact of machine learning today (used as assistance in medicine, hiring process, bank loans…). Besides, real world problems usually do not fit the standard assumptions or frameworks of the most famous academic work (e.g., data quantity and quality). This course aims at providing insights and tools to address these practical aspects, based on mathematical concepts.
This course first emphasizes the gap between practice and classical theory (e.g., number of parameters), and reconciles them thanks to recent theoretical advances.
After a review of recent architectures, it studies visualization techniques, and checks that undesired biases present in the dataset (e.g., sensitivity to gender when matching CVs to job offers) are not reproduced.
It then investigates practical issues when training neural networks, in particular data quantity (small or big data), application to reinforcement learning and physical problems, and automatic hyper-parameter tuning.

## Themes

– Deep learning vs. classical machine learning and optimization

– Interpretability

– Architectures

– Small or noisy data: forms of weak supervision

– Modeling: deep learning and physics (exploiting known invariances, priors or physical properties)

– Generative models (GAN, VAE and Normalizing Flows) + Auto-ML / Auto-DeepLearning

– Guarantees? Generalization and formal proofs
